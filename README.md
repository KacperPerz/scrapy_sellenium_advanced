# Scrapy + Selenium: advanced solutions
The repository consists of 4 projects. Each of them tries to explain a different aspect of webscraping and webcrawling. While [scrapy](https://scrapy.org/) is fast,
asynchronous framework to scrape static webpages, [selenium](https://www.selenium.dev/) is predestined to handle js-generated websites. Depending on the problem, 
I used them alternately or together.  
## Scrapy: gathering data from multipages, arguments handling
![image](https://user-images.githubusercontent.com/34272444/179866126-af00746a-6c7c-45f1-9cd3-fef2c3cd3419.png)

The goal is to gather data about courses from [classcentral.com](https://www.classcentral.com/subjects). Courses are divided by categories.

We can simply specify a category in which we want to search by typing from command line:
```bash
scrapy crawl classcentral -a subject="<CATEGORY_NAME>"
```
To specify the output file:
```bash
scrapy crawl classcentral -a subject="<CATEGORY_NAME>" -o "<OUTPUT_NAME>.json/.csv"
```
If we want to search through all categories ignore the part with category name:
```bash
scrapy crawl classcentral
```

## Dynamic website: handling forms
![image](https://user-images.githubusercontent.com/34272444/179867634-cf57de6b-0686-46d2-b0b3-e1c15b1236ac.png)

This script deals with forms. After selecting a region on a map, program tries to fill the form and then send it by post method and after receiving the reply 
starts going into each record.

![image](https://user-images.githubusercontent.com/34272444/179868067-904a6ae0-2271-4b86-8e96-20a5a870f1c3.png)

The last link of the chain is to get details from inside of record. Finally the script checks whether an "Agents" button exists and if it does, then simply gets data
from this section.

![image](https://user-images.githubusercontent.com/34272444/179868240-f587354b-8c00-49e5-8d07-74583154fb69.png)

## Linkedin: sign in and scrape
The selenium (or other framework created to handle javascript) is undeniably indispensable. We load email/phone-number and password field and fulfill them. Then,
after we signed up correctly, we search in google for queries that we are interested in.
In parameters.py we specify google's query, the name of final file, email/phone-number and password.

```python
search_query = '' # your google query, i.e 'site:linkedin.com/in/ AND "python developer" AND "New York"'
result_file = 'output.csv'

username = '' # your linkedin mail or phone number
password = '' # your linkedin password
```

As long as we deal with clickable buttons and the necessity of fulfilling inputs - we need webdriver's solution.

![image](https://user-images.githubusercontent.com/34272444/179869326-bf91649a-af96-475d-bf10-478732ab6af3.png)

When you enter the userâ€™s website, we can use faster scrapy to get static data.

## From below: working with API

Not every time we have this comfort to work on pure html or html generated by javascript. Sometimes we need to use API, as the natural way of gathering provided
data.

![image](https://user-images.githubusercontent.com/34272444/179870178-28255b4b-f7a2-4b88-ac1a-ff26fa9e00c1.png)

When we watch the traffic on the network, we notice that the data is being loaded from the server.

```python
price_api_url = 'https://www.asos.com/api/product/catalogue/v3/stockprice?productIds=' + product_id + '&store=COM&currency=GBP'
yield Request(price_api_url, meta={'product_name': product_name}, callback=self.parse_shoe_price)
```

![image](https://user-images.githubusercontent.com/34272444/179870639-6bfb3449-1d30-4d58-8ffa-5ce963c4fad1.png)


